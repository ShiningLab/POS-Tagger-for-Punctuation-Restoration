{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f548b2fc-2f49-4127-b334-eb3e35129d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "__author__ = 'Shining'\n",
    "__email__ = 'shining.shi@alibaba-inc.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675036f4-1478-491d-8c53-cef5ee17f958",
   "metadata": {},
   "source": [
    "# POS-Tagger-for-Punctuation-Restoration\n",
    "Demo for the paper [Incorporating External POS Tagger for Punctuation Restoration](https://arxiv.org/abs/2106.06731) in proceedings of the [*2021 Conference of the International Speech Communication Association (INTERSPEECH)*](https://www.interspeech2021.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064964b-b418-4af4-9b3b-441caf1328a7",
   "metadata": {},
   "source": [
    "***Note***\n",
    "1. Please download a sample checkpoint from ***[here](https://drive.google.com/drive/folders/1mJ0ReoAToiENozPLhygJiX2FwVjcBNl7?usp=sharing)***, and save as ***main/res/check_points/funnel-transformer-xlarge/xfmr/pretrained/random/linear.pt***.\n",
    "2. The file path of the checkpoint depends on your ***config***.\n",
    "2. For errors regarding ipywidgets, please try:  \n",
    "```Python\n",
    "pip install ipywidgets\n",
    "```\n",
    "3. It takes time to download resources for the first time for flair pos tagger and language model.\n",
    "4. The downloading progress bar may not work in jupyter, try to pass train.py first.\n",
    "5. The model proposed in the original paper is trained and evaluated with a maximum sequence length of 256. Thus, the demo results may change according to the sequence length of your test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b487e13-40cc-4030-8d66-16b1d941c31b",
   "metadata": {},
   "source": [
    "# Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997a06c8-2044-48cd-8a89-505686de2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# public\n",
    "from flair.data import Sentence\n",
    "import torch\n",
    "from torch.utils import data as torch_data\n",
    "from tqdm import tqdm\n",
    "# private\n",
    "from train import Restorer\n",
    "from src.utils import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25d907-6fbe-48d1-bb90-31e6c9531800",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b462955-a6ab-4119-a7c0-7e9a25153565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch_data.Dataset):\n",
    "    \"\"\"docstring for Dataset\"\"\"\n",
    "    def __init__(self, xs, y_masks, y_tags):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.xs = xs\n",
    "        self.y_masks = y_masks\n",
    "        self.y_tags = y_tags\n",
    "        self.data_size = len(self.xs)\n",
    "\n",
    "    def __len__(self): \n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.y_masks[idx], self.y_tags[idx]\n",
    "\n",
    "def collate_fn(data): \n",
    "    # a customized collate function used in the data loader \n",
    "    data.sort(key=len, reverse=True)\n",
    "    raw_xs, raw_y_masks, raw_y_tags = zip(*data)\n",
    "    xs, x_masks, y_masks, y_tags = [], [], [], []\n",
    "    for i in range(len(raw_xs)):\n",
    "        x = raw_xs[i]\n",
    "        y_mask = raw_y_masks[i]\n",
    "        y_tag = raw_y_tags[i]\n",
    "        # padding\n",
    "        if len(x) < re.config.max_seq_len:\n",
    "            diff_len = re.config.max_seq_len - len(x)\n",
    "            x += [re.config.PAD_TOKEN for _ in range(diff_len)]\n",
    "            y_mask += [0 for _ in range(diff_len)]\n",
    "            y_tag += [re.config.X_TAG for _ in range(diff_len)]\n",
    "        x_mask = [0 if token == re.config.PAD_TOKEN else 1 for token in x]\n",
    "        x = re.tokenizer.convert_tokens_to_ids(x)\n",
    "        xs.append(x)\n",
    "        x_masks.append(x_mask)\n",
    "        y_masks.append(y_mask)\n",
    "        y_tag = re.pos_tagger.tag_dictionary.get_idx_for_items(y_tag)\n",
    "        y_tags.append(y_tag)\n",
    "    return (raw_xs, raw_y_masks, raw_y_tags), (xs, x_masks, y_masks, y_tags)\n",
    "\n",
    "def translate(seq: list, trans_dict: dict) -> list: \n",
    "    return [trans_dict[token] for token in seq]\n",
    "\n",
    "def post_process(xs, x_masks, y_masks, ys_, tokenizer, config):\n",
    "    # remove padding\n",
    "    xs, x_masks = (i.cpu().detach().numpy().tolist() for i in (xs, x_masks))\n",
    "    ys_ = torch.argmax(ys_, dim=2).cpu().detach().numpy().tolist()\n",
    "    xs_lens = [sum(x_mask) for x_mask in x_masks]\n",
    "    xs = [x[:l] for x, l in zip(xs, xs_lens)]\n",
    "    y_masks = [y_mask[:l] for y_mask, l in zip(y_masks.tolist(), xs_lens)]\n",
    "    ys_ = [y_[:l] for y_, l in zip(ys_, xs_lens)]\n",
    "    xs = [tokenizer.convert_ids_to_tokens(x) for x in xs]\n",
    "    ys_ = [translate(y_, config.idx2label_dict) for y_ in ys_]\n",
    "    return xs, y_masks, ys_\n",
    "\n",
    "def restore_pun(x, y_mask, y):\n",
    "    for i in range(len(x)):\n",
    "        if y_mask[i] and y[i] != re.config.NORMAL_TOKEN:\n",
    "            x.insert(i+1, y[i])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6e888-748f-4f6e-8ecc-da8c0cb2d0cc",
   "metadata": {},
   "source": [
    "# Demo\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb38d56-fe1b-49cb-af62-63bb5877b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-14 03:55:05,040 loading file /Users/shining/.flair/models/upos-english-fast/b631371788604e95f27b6567fe7220e4a7e8d03201f3d862e6204dbf90f9f164.0afb95b43b32509bf4fcc3687f7c64157d8880d08f813124c1bd371c3d8ee3f7\n"
     ]
    }
   ],
   "source": [
    "# setup model class\n",
    "re = Restorer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c083f921-a636-4c20-ab03-cf887413c322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored from /Users/shining/Library/Mobile Documents/com~apple~CloudDocs/Desktop/ShiningLab/project/Punctuation Restoration/exp/main/res/check_points/funnel-transformer-xlarge/xfmr/pretrained/random/linear.pt.\n"
     ]
    }
   ],
   "source": [
    "# restore model from checkpoint\n",
    "model = pipeline.pick_model(re.config)\n",
    "checkpoint_to_load =  torch.load(\n",
    "    re.config.SAVE_POINT\n",
    "    , map_location=re.config.device)\n",
    "model.load_state_dict(checkpoint_to_load['model'])\n",
    "model.eval()\n",
    "print('Model restored from {}.'.format(re.config.SAVE_POINT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2f88c-0264-45c4-bee4-124eab421b5e",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eff7a0d-ef41-4096-b388-f07941712843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case\n",
    "x1 = \"it can be a very complicated thing the ocean \\\n",
    "and it can be a very complicated thing what human health is and \\\n",
    "bringing those two together might seem a very daunting task\"\n",
    "\n",
    "x2 = \"i 'm as a font or more precisely a high-functioning autistic \\\n",
    "savant it 's a rare condition and rarer still when accompanied as in \\\n",
    "my case by self-awareness and a mastery of language very often when i \\\n",
    "meet someone and they've learned is about me there 's a certain kind of awkwardness\"\n",
    "\n",
    "raw_xs = [x1, x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52208c38-e4cf-4cb5-bb34-39e6e6bb6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "xs, y_masks = [], []\n",
    "for i in range(len(raw_xs)):\n",
    "    x, y_mask = [], []\n",
    "    for word in raw_xs[i].split():\n",
    "        tokens = re.tokenizer.tokenize(word)\n",
    "        for j in range(len(tokens)-1):\n",
    "            x.append(tokens[j])\n",
    "            y_mask.append(0)\n",
    "        x.append(tokens[-1])\n",
    "        y_mask.append(1)\n",
    "    xs.append(x)\n",
    "    y_masks.append(y_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad339462-9cf6-443a-a616-9d6a99543c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "y_tags = []\n",
    "for x, y_mask in zip(xs, y_masks):\n",
    "    y_tag = []\n",
    "    sent = Sentence(x)\n",
    "    re.pos_tagger.predict(sent)\n",
    "    tags = [e.tag for e in sent.get_spans('pos')]\n",
    "    for t, m in zip(tags, y_mask):\n",
    "        y_tag.append(t) if m else y_tag.append(re.config.X_TAG)\n",
    "    y_tags.append(y_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea8709c5-9657-4114-a1a4-88784fef058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing\n",
    "data = (xs, y_masks, y_tags)\n",
    "xs, y_masks, y_tags = [], [], []\n",
    "for tokens, masks, tags in zip(*data):\n",
    "    x, y_mask, y_tag = [re.config.BOS_TOKEN], [0], [re.config.X_TAG]\n",
    "    for token, mask, tag in zip(tokens, masks, tags):\n",
    "        x.append(token)\n",
    "        y_mask.append(mask)\n",
    "        y_tag.append(tag)\n",
    "    x.append(re.config.EOS_TOKEN)\n",
    "    y_mask.append(0)\n",
    "    y_tag.append(re.config.X_TAG)\n",
    "    xs.append(x)\n",
    "    y_masks.append(y_mask)\n",
    "    y_tags.append(y_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b17616df-cb7d-4e67-be9c-410675698299",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(xs, y_masks, y_tags)\n",
    "dataloader = torch_data.DataLoader(\n",
    "    dataset\n",
    "    , batch_size=re.config.batch_size\n",
    "    , collate_fn=collate_fn\n",
    "    , shuffle=False\n",
    "    , num_workers=re.config.num_workers\n",
    "    , pin_memory=re.config.pin_memory\n",
    "    , drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb28f1-c881-47c0-a9af-162a0f0ee856",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c9cf3b-1c83-4ffe-b351-af5cb1c7af99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                   | 0/1 [00:00<?, ?it/s]/Users/shining/miniconda3/envs/pun/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.17s/it]\n"
     ]
    }
   ],
   "source": [
    "all_xs, all_ys, all_y_masks, all_ys_ = [], [], [], []\n",
    "dl = tqdm(dataloader)\n",
    "with torch.no_grad():\n",
    "    for data_pair in dl:\n",
    "        raw_data, data = data_pair\n",
    "        xs, x_masks, y_masks, y_tags = (torch.LongTensor(_).to(re.config.device) for _ in data)\n",
    "        ys_ = model(xs, x_masks, y_tags)\n",
    "        \n",
    "#         print(raw_data[0][0])\n",
    "#         print(len(xs[0])) \n",
    "#         print(re.tokenizer.convert_ids_to_tokens(xs[0]))\n",
    "#         print(x_masks[0])\n",
    "#         print([re.config.idx2label_dict[label] for label in ys[0].tolist()])\n",
    "#         print(y_masks[0])\n",
    "#         print(pipeline.translate(y_tags[0], re.pos_tagger.tag_dictionary.idx2item))\n",
    "        \n",
    "        xs, y_masks, ys_ = post_process(xs, x_masks, y_masks, ys_, re.tokenizer, re.config)\n",
    "        all_xs += xs\n",
    "        all_y_masks += y_masks\n",
    "        all_ys_ += ys_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7201b7-99e0-4588-80d7-8cd1dd1e29d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: ['<s>', 'it', 'can', 'be', 'a', 'very', 'complicated', 'thing', 'the', 'ocean', 'and', 'it', 'can', 'be', 'a', 'very', 'complicated', 'thing', 'what', 'human', 'health', 'is', 'and', 'bringing', 'those', 'two', 'together', 'might', 'seem', 'a', 'very', 'da', '##unt', '##ing', 'task', '</s>']\n",
      "y_mask: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "y_: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'COMMA', 'O', 'COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'COMMA', 'O', 'O', 'O', 'PERIOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERIOD', 'O']\n",
      "\n",
      "x: ['<s>', 'i', \"'\", 'm', 'as', 'a', 'font', 'or', 'more', 'precisely', 'a', 'high', '-', 'functioning', 'au', '##tist', '##ic', 'sava', '##nt', 'it', \"'\", 's', 'a', 'rare', 'condition', 'and', 'rare', '##r', 'still', 'when', 'accompanied', 'as', 'in', 'my', 'case', 'by', 'self', '-', 'awareness', 'and', 'a', 'mastery', 'of', 'language', 'very', 'often', 'when', 'i', 'meet', 'someone', 'and', 'they', \"'\", 've', 'learned', 'is', 'about', 'me', 'there', \"'\", 's', 'a', 'certain', 'kind', 'of', 'awkward', '##ness', '</s>']\n",
      "y_mask: [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "y_: ['O', 'O', 'O', 'COMMA', 'O', 'O', 'COMMA', 'O', 'O', 'COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERIOD', 'O', 'O', 'O', 'O', 'O', 'COMMA', 'O', 'O', 'O', 'O', 'O', 'COMMA', 'O', 'O', 'O', 'COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERIOD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'COMMA', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_xs)):\n",
    "    print('x: {}'.format(all_xs[i]))\n",
    "    print('y_mask: {}'.format(all_y_masks[i]))\n",
    "    print('y_: {}'.format(all_ys_[i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b2f40d-5565-4917-bddd-11f05738bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Test 0***\n",
      "<s>\tO\n",
      "it\tO\n",
      "can\tO\n",
      "be\tO\n",
      "a\tO\n",
      "very\tO\n",
      "complicated\tO\n",
      "thing\tCOMMA\n",
      "the\tO\n",
      "ocean\tCOMMA\n",
      "and\tO\n",
      "it\tO\n",
      "can\tO\n",
      "be\tO\n",
      "a\tO\n",
      "very\tO\n",
      "complicated\tO\n",
      "thing\tCOMMA\n",
      "what\tO\n",
      "human\tO\n",
      "health\tO\n",
      "is\tPERIOD\n",
      "and\tO\n",
      "bringing\tO\n",
      "those\tO\n",
      "two\tO\n",
      "together\tO\n",
      "might\tO\n",
      "seem\tO\n",
      "a\tO\n",
      "very\tO\n",
      "da\tO\n",
      "##unt\tO\n",
      "##ing\tO\n",
      "task\tPERIOD\n",
      "</s>\tO\n",
      "\n",
      "***Test 1***\n",
      "<s>\tO\n",
      "i\tO\n",
      "'\tO\n",
      "m\tCOMMA\n",
      "as\tO\n",
      "a\tO\n",
      "font\tCOMMA\n",
      "or\tO\n",
      "more\tO\n",
      "precisely\tCOMMA\n",
      "a\tO\n",
      "high\tO\n",
      "-\tO\n",
      "functioning\tO\n",
      "au\tO\n",
      "##tist\tO\n",
      "##ic\tO\n",
      "sava\tO\n",
      "##nt\tPERIOD\n",
      "it\tO\n",
      "'\tO\n",
      "s\tO\n",
      "a\tO\n",
      "rare\tO\n",
      "condition\tCOMMA\n",
      "and\tO\n",
      "rare\tO\n",
      "##r\tO\n",
      "still\tO\n",
      "when\tO\n",
      "accompanied\tCOMMA\n",
      "as\tO\n",
      "in\tO\n",
      "my\tO\n",
      "case\tCOMMA\n",
      "by\tO\n",
      "self\tO\n",
      "-\tO\n",
      "awareness\tO\n",
      "and\tO\n",
      "a\tO\n",
      "mastery\tO\n",
      "of\tO\n",
      "language\tPERIOD\n",
      "very\tO\n",
      "often\tO\n",
      "when\tO\n",
      "i\tO\n",
      "meet\tO\n",
      "someone\tO\n",
      "and\tO\n",
      "they\tO\n",
      "'\tO\n",
      "ve\tO\n",
      "learned\tO\n",
      "is\tO\n",
      "about\tO\n",
      "me\tCOMMA\n",
      "there\tO\n",
      "'\tO\n",
      "s\tO\n",
      "a\tO\n",
      "certain\tO\n",
      "kind\tO\n",
      "of\tO\n",
      "awkward\tO\n",
      "##ness\tO\n",
      "</s>\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_xs)):\n",
    "    print('***Test {}***'.format(i))\n",
    "    for word, pun in zip(all_xs[i], all_ys_[i]):\n",
    "        print('{}\\t{}'.format(word, pun))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1c349-47d2-4181-a1e9-420a7106fd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
